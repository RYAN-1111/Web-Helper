{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adae561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ryanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ryanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ryanr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline \n",
    "import seaborn as sns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73798e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the initial dataset\n",
    "initial_dataset = pd.read_csv('amazonreviews.tsv', delimiter='\\t')\n",
    "\n",
    "# Perform data augmentation using NLTK\n",
    "augmented_data = []\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "for text in initial_dataset['review']:\n",
    "    tokens = word_tokenize(text)\n",
    "    augmented_tokens = []\n",
    "    \n",
    "    # Synonym Replacement\n",
    "    for token in tokens:\n",
    "        synsets = wordnet.synsets(token)\n",
    "        if synsets:\n",
    "            synonyms = [synset.lemmas()[0].name() for synset in synsets]\n",
    "            augmented_token = synonyms[0] if len(synonyms) == 1 else synonyms[1]\n",
    "        else:\n",
    "            augmented_token = token\n",
    "        augmented_tokens.append(augmented_token)\n",
    "    \n",
    "    # Random Swap\n",
    "    n = len(augmented_tokens)\n",
    "    for i in range(n):\n",
    "        if i < n-1 and i % 2 == 0:\n",
    "            augmented_tokens[i], augmented_tokens[i+1] = augmented_tokens[i+1], augmented_tokens[i]\n",
    "    \n",
    "    # Random Deletion\n",
    "    augmented_tokens = [token for token in augmented_tokens if token.lower() not in stopwords_set or token.lower() == 'not']\n",
    "    augmented_text = ' '.join(augmented_tokens)\n",
    "    augmented_data.append(augmented_text)\n",
    "\n",
    "# Create a new DataFrame with augmented data\n",
    "augmented_dataset = pd.DataFrame({'label': initial_dataset['label'],'review': augmented_data})\n",
    "\n",
    "# Save the augmented dataset to a file\n",
    "augmented_dataset.to_csv('augmented_dataset.tsv', sep='\\t', index=False)\n",
    "\n",
    "# Merge the two datasets into a single file\n",
    "merged_dataset = pd.concat([initial_dataset, augmented_dataset], ignore_index=True)\n",
    "\n",
    "# Save the merged dataset to a file\n",
    "merged_dataset.to_csv('merged_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5929983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   pos  Stuning even for the non-gamer: This sound tra...\n",
       "1   pos  The best soundtrack ever to anything.: I'm rea...\n",
       "2   pos  Amazing!: This soundtrack is my favorite music...\n",
       "3   pos  Excellent Soundtrack: I truly like this soundt...\n",
       "4   pos  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('merged_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c4dbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the selected dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "652d6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14564a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing null values\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#removing empty strings \n",
    "blanks = [] \n",
    "for i,lb,rv in df.itertuples():  \n",
    "    if type(rv)==str:            \n",
    "        if rv.isspace():         \n",
    "            blanks.append(i)     \n",
    "        \n",
    "df.drop(blanks, inplace=True)\n",
    "\n",
    "#split data-set to train and test\n",
    "X=df['review']\n",
    "y=df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a4ae57",
   "metadata": {},
   "source": [
    "# Model 1 :- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1751bba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_01155_row0_col0, #T_01155_row0_col2, #T_01155_row1_col1, #T_01155_row1_col2, #T_01155_row2_col2, #T_01155_row2_col3, #T_01155_row3_col2, #T_01155_row4_col2 {\n",
       "  background-color: #ebf3eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_01155_row0_col1, #T_01155_row1_col0, #T_01155_row2_col0, #T_01155_row3_col0, #T_01155_row3_col3, #T_01155_row4_col0, #T_01155_row4_col3 {\n",
       "  background-color: #008000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_01155_row0_col3, #T_01155_row2_col1, #T_01155_row3_col1, #T_01155_row4_col1 {\n",
       "  background-color: #75b975;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_01155_row1_col3 {\n",
       "  background-color: #76ba76;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_01155\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_01155_level0_col0\" class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th id=\"T_01155_level0_col1\" class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th id=\"T_01155_level0_col2\" class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th id=\"T_01155_level0_col3\" class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_01155_level0_row0\" class=\"row_heading level0 row0\" >neg</th>\n",
       "      <td id=\"T_01155_row0_col0\" class=\"data row0 col0\" >0.870000</td>\n",
       "      <td id=\"T_01155_row0_col1\" class=\"data row0 col1\" >0.890000</td>\n",
       "      <td id=\"T_01155_row0_col2\" class=\"data row0 col2\" >0.880000</td>\n",
       "      <td id=\"T_01155_row0_col3\" class=\"data row0 col3\" >3316.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_01155_level0_row1\" class=\"row_heading level0 row1\" >pos</th>\n",
       "      <td id=\"T_01155_row1_col0\" class=\"data row1 col0\" >0.880000</td>\n",
       "      <td id=\"T_01155_row1_col1\" class=\"data row1 col1\" >0.870000</td>\n",
       "      <td id=\"T_01155_row1_col2\" class=\"data row1 col2\" >0.880000</td>\n",
       "      <td id=\"T_01155_row1_col3\" class=\"data row1 col3\" >3284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_01155_level0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "      <td id=\"T_01155_row2_col0\" class=\"data row2 col0\" >0.880000</td>\n",
       "      <td id=\"T_01155_row2_col1\" class=\"data row2 col1\" >0.880000</td>\n",
       "      <td id=\"T_01155_row2_col2\" class=\"data row2 col2\" >0.880000</td>\n",
       "      <td id=\"T_01155_row2_col3\" class=\"data row2 col3\" >0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_01155_level0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "      <td id=\"T_01155_row3_col0\" class=\"data row3 col0\" >0.880000</td>\n",
       "      <td id=\"T_01155_row3_col1\" class=\"data row3 col1\" >0.880000</td>\n",
       "      <td id=\"T_01155_row3_col2\" class=\"data row3 col2\" >0.880000</td>\n",
       "      <td id=\"T_01155_row3_col3\" class=\"data row3 col3\" >6600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_01155_level0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "      <td id=\"T_01155_row4_col0\" class=\"data row4 col0\" >0.880000</td>\n",
       "      <td id=\"T_01155_row4_col1\" class=\"data row4 col1\" >0.880000</td>\n",
       "      <td id=\"T_01155_row4_col2\" class=\"data row4 col2\" >0.880000</td>\n",
       "      <td id=\"T_01155_row4_col3\" class=\"data row4 col3\" >6600.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f664fcd990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model=Pipeline([('tfidf', TfidfVectorizer(lowercase=False)),( 'clf',LogisticRegression(solver='lbfgs'))])\n",
    "lr_model.fit(X_train,y_train)\n",
    "\n",
    "predictions= lr_model.predict(X_test)\n",
    "report = classification_report(y_test,predictions, output_dict=True)\n",
    "\n",
    "df_report = pd.DataFrame(report).transpose().round(2)\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "df_report.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29a998",
   "metadata": {},
   "source": [
    "# Model 2 :- Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed2ff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1761b_row0_col0, #T_1761b_row0_col1, #T_1761b_row0_col2, #T_1761b_row1_col0, #T_1761b_row1_col1, #T_1761b_row1_col2, #T_1761b_row2_col0, #T_1761b_row2_col1, #T_1761b_row2_col2, #T_1761b_row2_col3, #T_1761b_row3_col0, #T_1761b_row3_col1, #T_1761b_row3_col2, #T_1761b_row4_col0, #T_1761b_row4_col1, #T_1761b_row4_col2 {\n",
       "  background-color: #ebf3eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_1761b_row0_col3 {\n",
       "  background-color: #75b975;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1761b_row1_col3 {\n",
       "  background-color: #76ba76;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_1761b_row3_col3, #T_1761b_row4_col3 {\n",
       "  background-color: #008000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1761b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1761b_level0_col0\" class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th id=\"T_1761b_level0_col1\" class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th id=\"T_1761b_level0_col2\" class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th id=\"T_1761b_level0_col3\" class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1761b_level0_row0\" class=\"row_heading level0 row0\" >neg</th>\n",
       "      <td id=\"T_1761b_row0_col0\" class=\"data row0 col0\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row0_col1\" class=\"data row0 col1\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row0_col2\" class=\"data row0 col2\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row0_col3\" class=\"data row0 col3\" >3316.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1761b_level0_row1\" class=\"row_heading level0 row1\" >pos</th>\n",
       "      <td id=\"T_1761b_row1_col0\" class=\"data row1 col0\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row1_col1\" class=\"data row1 col1\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row1_col2\" class=\"data row1 col2\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row1_col3\" class=\"data row1 col3\" >3284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1761b_level0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "      <td id=\"T_1761b_row2_col0\" class=\"data row2 col0\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row2_col1\" class=\"data row2 col1\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row2_col2\" class=\"data row2 col2\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row2_col3\" class=\"data row2 col3\" >0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1761b_level0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "      <td id=\"T_1761b_row3_col0\" class=\"data row3 col0\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row3_col1\" class=\"data row3 col1\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row3_col2\" class=\"data row3 col2\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row3_col3\" class=\"data row3 col3\" >6600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1761b_level0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "      <td id=\"T_1761b_row4_col0\" class=\"data row4 col0\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row4_col1\" class=\"data row4 col1\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row4_col2\" class=\"data row4 col2\" >0.920000</td>\n",
       "      <td id=\"T_1761b_row4_col3\" class=\"data row4 col3\" >6600.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f664f382d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the model\n",
    "my_model=Pipeline([('tfidf', TfidfVectorizer()),('classifier',LinearSVC())])\n",
    "my_model.fit(X_train,y_train)\n",
    "\n",
    "predictions= my_model.predict(X_test)\n",
    "report = classification_report(y_test,predictions, output_dict=True)\n",
    "\n",
    "df_report = pd.DataFrame(report).transpose().round(2)\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "df_report.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f63a2d",
   "metadata": {},
   "source": [
    "# Model 3 :- Vader's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0db0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled dataset\n",
    "data= pd.read_csv('amazonreviews.tsv', sep='\\t')\n",
    "\n",
    "# Initialize the Vader sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create empty lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over each row in the dataset\n",
    "for index, row in data.iterrows():\n",
    "    text = row['review']\n",
    "    true_sentiment = row['label']\n",
    "    \n",
    "    # Get the predicted sentiment using the Vader model\n",
    "    scores = vader.polarity_scores(text)\n",
    "    predicted_sentiment = 'pos' if scores['compound'] >= 0 else 'neg'\n",
    "    \n",
    "    # Append the true and predicted labels to the respective lists\n",
    "    true_labels.append(true_sentiment)\n",
    "    predicted_labels.append(predicted_sentiment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfe6b053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_17ae9_row0_col0, #T_17ae9_row1_col1, #T_17ae9_row1_col2, #T_17ae9_row3_col3, #T_17ae9_row4_col3 {\n",
       "  background-color: #008000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_17ae9_row0_col1, #T_17ae9_row0_col2, #T_17ae9_row1_col0, #T_17ae9_row2_col3 {\n",
       "  background-color: #ebf3eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_17ae9_row0_col3 {\n",
       "  background-color: #73b873;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_17ae9_row1_col3 {\n",
       "  background-color: #78bb78;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_17ae9_row2_col0 {\n",
       "  background-color: #a0cea0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_17ae9_row2_col1, #T_17ae9_row3_col1, #T_17ae9_row4_col1 {\n",
       "  background-color: #78bb78;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_17ae9_row2_col2 {\n",
       "  background-color: #56aa56;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_17ae9_row3_col0, #T_17ae9_row4_col0 {\n",
       "  background-color: #75b975;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_17ae9_row3_col2, #T_17ae9_row4_col2 {\n",
       "  background-color: #6bb46b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_17ae9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_17ae9_level0_col0\" class=\"col_heading level0 col0\" >precision</th>\n",
       "      <th id=\"T_17ae9_level0_col1\" class=\"col_heading level0 col1\" >recall</th>\n",
       "      <th id=\"T_17ae9_level0_col2\" class=\"col_heading level0 col2\" >f1-score</th>\n",
       "      <th id=\"T_17ae9_level0_col3\" class=\"col_heading level0 col3\" >support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_17ae9_level0_row0\" class=\"row_heading level0 row0\" >neg</th>\n",
       "      <td id=\"T_17ae9_row0_col0\" class=\"data row0 col0\" >0.860000</td>\n",
       "      <td id=\"T_17ae9_row0_col1\" class=\"data row0 col1\" >0.520000</td>\n",
       "      <td id=\"T_17ae9_row0_col2\" class=\"data row0 col2\" >0.640000</td>\n",
       "      <td id=\"T_17ae9_row0_col3\" class=\"data row0 col3\" >5097.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17ae9_level0_row1\" class=\"row_heading level0 row1\" >pos</th>\n",
       "      <td id=\"T_17ae9_row1_col0\" class=\"data row1 col0\" >0.640000</td>\n",
       "      <td id=\"T_17ae9_row1_col1\" class=\"data row1 col1\" >0.910000</td>\n",
       "      <td id=\"T_17ae9_row1_col2\" class=\"data row1 col2\" >0.750000</td>\n",
       "      <td id=\"T_17ae9_row1_col3\" class=\"data row1 col3\" >4903.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17ae9_level0_row2\" class=\"row_heading level0 row2\" >accuracy</th>\n",
       "      <td id=\"T_17ae9_row2_col0\" class=\"data row2 col0\" >0.710000</td>\n",
       "      <td id=\"T_17ae9_row2_col1\" class=\"data row2 col1\" >0.710000</td>\n",
       "      <td id=\"T_17ae9_row2_col2\" class=\"data row2 col2\" >0.710000</td>\n",
       "      <td id=\"T_17ae9_row2_col3\" class=\"data row2 col3\" >0.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17ae9_level0_row3\" class=\"row_heading level0 row3\" >macro avg</th>\n",
       "      <td id=\"T_17ae9_row3_col0\" class=\"data row3 col0\" >0.750000</td>\n",
       "      <td id=\"T_17ae9_row3_col1\" class=\"data row3 col1\" >0.710000</td>\n",
       "      <td id=\"T_17ae9_row3_col2\" class=\"data row3 col2\" >0.700000</td>\n",
       "      <td id=\"T_17ae9_row3_col3\" class=\"data row3 col3\" >10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17ae9_level0_row4\" class=\"row_heading level0 row4\" >weighted avg</th>\n",
       "      <td id=\"T_17ae9_row4_col0\" class=\"data row4 col0\" >0.750000</td>\n",
       "      <td id=\"T_17ae9_row4_col1\" class=\"data row4 col1\" >0.710000</td>\n",
       "      <td id=\"T_17ae9_row4_col2\" class=\"data row4 col2\" >0.700000</td>\n",
       "      <td id=\"T_17ae9_row4_col3\" class=\"data row4 col3\" >10000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f6555028d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = classification_report(true_labels,predicted_labels,output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose().round(2)\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "df_report.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f23d0b",
   "metadata": {},
   "source": [
    "# Pre Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b29896",
   "metadata": {},
   "source": [
    "1) Logistic Regression Model\n",
    "\n",
    "Accuracy: 85% Precision: 87% Recall: 85% F1-score: 85%\n",
    "                \n",
    "2) Linear SVC (Support Vector Classifier)\n",
    "\n",
    "Accuracy: 87% Precision: 89% Recall: 87% F1-score: 88%\n",
    "                \n",
    "3) Vader's Model (VADER Sentiment Intensity Analyzer)\n",
    "\n",
    "Accuracy: 70% Precision: 64% Recall: 91% F1-score: 75%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7918b",
   "metadata": {},
   "source": [
    "# Post Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970dbe7",
   "metadata": {},
   "source": [
    "1) Logistic Regression Model\n",
    "\n",
    "Accuracy: 88% Precision: 88% Recall: 89% F1-score: 88%\n",
    "                \n",
    "2) Linear SVC (Support Vector Classifier)\n",
    "\n",
    "Accuracy: 92% Precision: 92% Recall: 92% F1-score: 92%\n",
    "                \n",
    "3) Vader's Model (VADER Sentiment Intensity Analyzer)\n",
    "\n",
    "Accuracy: 71% Precision: 86% Recall: 91% F1-score: 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89403471",
   "metadata": {},
   "source": [
    "# Recommendation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a070f3",
   "metadata": {},
   "source": [
    "The Support Vector Classifier (SVC) is better for sentiment analysis due to its ability to capture non-linear relationships in text data, handle high-dimensional feature spaces effectively, and handle imbalanced datasets. It is robust to outliers and can tolerate misclassifications with a soft margin. Additionally, SVC can be extended to handle non-linear sentiment analysis tasks in various data types, making it a versatile choice for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904ca91",
   "metadata": {},
   "source": [
    "Data augmentation can initially be beneficial, continuously relying solely on data augmentation may not be the most effective approach to improving model accuracy. Instead, consider strategies such as increasing the dataset size, improving data quality, and exploring advanced feature engineering techniques or alternative models. These approaches can further enhance the accuracy of the models beyond what data augmentation alone can achieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
